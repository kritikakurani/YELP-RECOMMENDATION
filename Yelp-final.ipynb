{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Collection </h1>\n",
    "\n",
    "Data Source: Yelp Dataset Challenge https://www.yelp.com/dataset_challenge\n",
    "\n",
    "The original file is too large, we seperate the original json file into smaller file, each file only include one feature.\n",
    "\n",
    "Below is an example how we do data extraction. The json file is too large and it is NOT uploaded to GitHub. We run data collection on Brazos Cluster.\n",
    "\n",
    "See more in ./preprocess/data-extraction/DATA.README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PreProcess = False\n",
    "if PreProcess:\n",
    "    location = []\n",
    "    fInput = open(\"./yelp_academic_dataset_business.json\",'r')\n",
    "    for line in fInput:\n",
    "        txt = \"[\" + line.rstrip() + \"]\"\n",
    "        json_txt = json.loads(txt)\n",
    "        location.append([json_txt[0][\"business_id\"], json_txt[0][\"latitude\"], json_txt[0][\"longitude\"], json_txt[0][\"city\"], json_txt[0][\"state\"], json_txt[0][\"postal_code\"]])\n",
    "    fInput.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geo-clustering\n",
    "\n",
    "First we extracted location information from Yelp business data and clustered those business using DBSCAN library. We got 126 clusters. We choose a cluster Longtitude between -81.4 and -81.4, Latitude between 34.8 and 35.6.\n",
    "\n",
    "\n",
    "It takes some time to run clustering. \n",
    "See more details in ./preprocess/location-cluster/DBSCAN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RunCluster = False\n",
    "if RunCluster:\n",
    "    import pandas as pd, numpy as np, matplotlib.pyplot as plt, time\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    from sklearn import metrics\n",
    "    from geopy.distance import great_circle\n",
    "    from shapely.geometry import MultiPoint\n",
    "\n",
    "    kms_per_radian = 6371.0088\n",
    "    \n",
    "    df = pd.read_csv('./preprocess/location-cluster/local2.csv', encoding='utf-8')\n",
    "    df.head()\n",
    "\n",
    "    coords = df.as_matrix(columns=['lat', 'lon'])\n",
    "\n",
    "    # define epsilon as 1.5 kilometers to have a middle size cluster size\n",
    "    epsilon = 1.5 / kms_per_radian\n",
    "\n",
    "    db = DBSCAN(eps=epsilon, min_samples=1, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "    cluster_labels = db.labels_\n",
    "    num_clusters = len(set(cluster_labels))\n",
    "    clusters = pd.Series([coords[cluster_labels==n] for n in range(num_clusters)])\n",
    "\n",
    "    def get_centermost_point(cluster):\n",
    "        centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "        centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "        return tuple(centermost_point)\n",
    "    centermost_points = clusters.map(get_centermost_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./preprocess/fig/Distribution-All.png\">\n",
    "<img src=\"./preprocess/fig/Distribution-local2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review statistics\n",
    "\n",
    "Then we plot the user vs. number of review and business vs. number of review. Result showed below.\n",
    "\n",
    "Based on statistical results, user/business with review number less than 30 have a large proportion while these user/business review has little help with recommendation (only make the recomendation matrix sparse). So we decided to analysis user/business whose review number is more than 30.\n",
    "\n",
    "Since we will build a model based on ratings, reviews and relations. We also exclude user with less than 30 friends from our data to make the relation network more intensive. The final dataset used for our project is in ../new_5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_user = False\n",
    "plot_business = False\n",
    "\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "u = open('./preprocess/user+business_distribution/user-stat.out', 'r').read()\n",
    "user = eval(u)\n",
    "b = open('./preprocess/user+business_distribution/business-stat.out', 'r').read()\n",
    "business = eval(b)\n",
    "\n",
    "review_count  = user[0]\n",
    "average_stars = user[5]\n",
    "reviews = business[0]\n",
    "stars = business[1]\n",
    "\n",
    "if plot_user:\n",
    "    review_count_log = {}\n",
    "    for k in review_count: review_count_log[int(k/30)] = 0 \n",
    "    for k in review_count: review_count_log[int(k/30)] += review_count[k]\n",
    "    for k in review_count_log: review_count_log[k] = 1.0*math.log(review_count_log[k]+1.0)\n",
    "    xname = \"count(review)/30\"\n",
    "    yname = \"log( count(user) )\"\n",
    "    title_name = \"distribution of reviews per user\"\n",
    "    fig_name = \"user_review_count\"\n",
    "\n",
    "    lists = sorted(review_count_log.items()) \n",
    "    x, y = zip(*lists) \n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(xname)\n",
    "    plt.ylabel(yname)\n",
    "    plt.title(title_name)\n",
    "    plt.savefig(fig_name)\n",
    "    plt.close('all')\n",
    "\n",
    "if plot_business:\n",
    "    review_count_log = {}\n",
    "    for k in reviews: review_count_log[int(k/30)] = 0 \n",
    "    for k in reviews: review_count_log[int(k/30)] += reviews[k]\n",
    "    for k in review_count_log: review_count_log[k] = 1.0*math.log(review_count_log[k]+1.0)\n",
    "    xname = \"count(review)/30\"\n",
    "    yname = \"log( count(business) )\"\n",
    "    title_name = \"distribution of reviews per business\"\n",
    "    fig_name = \"business_review_count\"\n",
    "    lists = sorted(review_count_log.items()) \n",
    "    x, y = zip(*lists) \n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(xname)\n",
    "    plt.ylabel(yname)\n",
    "    plt.title(title_name)\n",
    "    plt.savefig(fig_name)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./preprocess/fig/user_review_count.png\">\n",
    "<img src=\"./preprocess/fig/business_review_count.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Recommendation System </h1>\n",
    "\n",
    "File 5k-data stores all business_ID, business_avg_rating, user_ID, and user_avg_rating.\n",
    "\n",
    "File 5k-relation stores all users and their friends.\n",
    "\n",
    "File 5k-review stores all business_ID(review_to), user_ID(review_by), and rating and review_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('./new_5k/5k-data', 'r')\n",
    "business_name = eval(f.readline())\n",
    "business_avg = eval(f.readline())\n",
    "user_name = eval(f.readline())\n",
    "user_avg = eval(f.readline())\n",
    "\n",
    "f = open('./new_5k/5k-relation', 'r').read()\n",
    "relation = eval(f)\n",
    "\n",
    "f = open('./new_5k/5k-review', 'r')\n",
    "review5k_business = eval(f.readline())\n",
    "review5k_user = eval(f.readline())\n",
    "review5k_rating = eval(f.readline())\n",
    "review5k_text = eval(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import math\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Parameters</h1>\n",
    "\n",
    "Below, we run the model once to show how our model works using the first 10% as test data and the rest 90% as train data. We alse use 10-fold method to select parameters. For more details about the 10-fold model, please refer to\n",
    "./bin/TopicMF-10fold-YY.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_user = []\n",
    "train_business = []\n",
    "train_rating = []\n",
    "train_text = []\n",
    "\n",
    "test_user = []\n",
    "test_business = []\n",
    "test_rating = []\n",
    "\n",
    "for r in xrange(len(review5k_rating)):\n",
    "    if r < len(review5k_rating)/10:\n",
    "        test_user.append(review5k_user[r])\n",
    "        test_business.append(review5k_business[r])\n",
    "        test_rating.append(review5k_rating[r])\n",
    "    else:\n",
    "        train_user.append(review5k_user[r])\n",
    "        train_business.append(review5k_business[r])\n",
    "        train_rating.append(review5k_rating[r])\n",
    "        train_text.append(review5k_text[r])\n",
    "        \n",
    "K_topic = 10\n",
    "Times = 50\n",
    "DocWord = 300\n",
    "DocTopic = 5\n",
    "\n",
    "lbd0 = 0.4 # Topic effect\n",
    "lbd1 = 0.5 # Similarity effect \n",
    "lbd2 = 0.5 # relation afftect\n",
    "lbd3 = 0.5 # VIP effect\n",
    "lbd4 = 0.5 # Topic effect in Tri-model\n",
    "lbd5 = 0.5 # VIP effect in Tri-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_user = len(user_avg)\n",
    "num_business = len(business_avg)\n",
    "num_train = len(train_rating)\n",
    "num_test = len(test_rating)\n",
    "\n",
    "mu = np.mean(train_rating)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = get_stop_words('en')\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "def prep(doc):\n",
    "    raw = doc.lower().replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    return (\" \").join(texts)\n",
    "\n",
    "from sklearn import linear_model\n",
    "def MLR(X, Y):\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(X, Y)\n",
    "    return reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Basic Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ubb = []\n",
    "for i in xrange(num_user):\n",
    "    Ubb.append([])\n",
    "    for j in xrange(num_business):\n",
    "        val = user_avg[i] + business_avg[j] - mu\n",
    "        if val < 1: val = 1\n",
    "        if val > 5: val = 5\n",
    "        Ubb[i].append(val) \n",
    "        \n",
    "UbbPd = []\n",
    "for r in xrange(num_test):\n",
    "    UbbPd.append(Ubb[test_user[r]][test_business[r]]) \n",
    "\n",
    "Ubb_rmse = mean_squared_error(test_rating, UbbPd)  \n",
    "\n",
    "print \"Ubb_rmse =\", Ubb_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BasicIn = []\n",
    "for i in xrange(num_user):\n",
    "    BasicIn.append([])\n",
    "    for j in xrange(num_business):\n",
    "        val = user_avg[i] + business_avg[j] - mu\n",
    "        if val < 1: val = 1\n",
    "        if val > 5: val = 5\n",
    "        BasicIn[i].append(val) \n",
    "        \n",
    "for r in xrange(num_train):\n",
    "    BasicIn[train_user[r]][train_business[r]] = train_rating[r]\n",
    "model = NMF(n_components=K_topic, init='random', random_state=0)\n",
    "U = model.fit_transform(BasicIn);\n",
    "V = model.components_;\n",
    "BasicOut = np.dot(U,V)\n",
    "BasicPd = []\n",
    "for r in xrange(num_test):\n",
    "    BasicPd.append(BasicOut[test_user[r]][test_business[r]]) \n",
    "\n",
    "BasicPd_rmse = mean_squared_error(test_rating, BasicPd)  \n",
    "\n",
    "print \"BasicPd_rmse =\", BasicPd_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Topic Model</h1>\n",
    "\n",
    "cell-1: For the topic model, we first preprocessed all reviews (including stemming, removing stop-words...). \n",
    "\n",
    "cell-2: For each business, we grouped all its reviews in training data as bag-of-words, and then use the LDA library to calculate the topic distribution for each business. To shoeter calculation time, we choose 5 topics.\n",
    "\n",
    "|      | topic 1| topic2 | topic3 | ... |\n",
    "|------|--------|--------|--------|-----|\n",
    "| doc1 | 0.0513 | 0.4686 | 0.0092 | ... |\n",
    "| doc2 | 0.0006 | 0.8601 | 0.0006 | ... |\n",
    "| doc3 | 0.9989 | 0.0003 | 0.0003 | ... |\n",
    "| ...  | ...    | ...    | ...    | ... |\n",
    "\n",
    "cell-3: For each user, we calculate his/her topic rating using linear regression based on his/her real-ratings in traing data substract Ubb. We mannually add six points to the linear regression in order to scale down the regression results. X1 = [0,0,0,0,0] Y1 = 0, X2 = [1,0,0,0,0] Y2 = 0, X3 = [0,1,0,0,0] Y3 = 0, X4 = [0,0,1,0,0] Y4 = 0, X5 = [0,0,0,1,0] Y5 = 0, X6 = [0,0,0,0,1] Y6 = 0\n",
    "\n",
    "\n",
    "$$delta = real_rating - mu - b_x - b_i$$\n",
    "$$predicted(rating_i−Ubb) = delta\\_reg = coef1*topic1 + coef2*topic2 + ...$$\n",
    "\n",
    "cell-4: calculation of rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print list(DocTopDist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Breview = [\"\"]*num_business\n",
    "for r in xrange(num_train):\n",
    "    Breview[train_business[r]] += prep(train_text[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=DocWord, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(Breview)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=DocTopic, max_iter=5, learning_method='online',learning_offset=50.,random_state=0)\n",
    "DocTopDist = lda.fit_transform(tf)\n",
    "print \"topic distribution for the first business is\"\n",
    "print list(DocTopDist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "\n",
    "# delta -> difference between rating and ubb [u*b], delta2 -> rating boolean\n",
    "delta = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "delta2 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "for r in xrange(num_train):\n",
    "    delta[train_user[r]][train_business[r]] = train_rating[r] - Ubb[train_user[r]][train_business[r]]\n",
    "    delta2[train_user[r]][train_business[r]] = 1\n",
    "\n",
    "deltaReg = []\n",
    "for i in xrange(num_user):\n",
    "    X = [[0,0,0,0,0], [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1]]; Y = [0,0,0,0,0,0]\n",
    "    for j in xrange(num_business):\n",
    "        if delta2[i][j]:\n",
    "            X.append(list(DocTopDist[j]))\n",
    "            Y.append(delta[i][j])\n",
    "    coef = MLR(X, Y)\n",
    "    deltaReg.append(np.dot(DocTopDist, coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "TopicIn = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "for i in xrange(num_user):\n",
    "    for j in xrange(num_business):\n",
    "        val = Ubb[i][j] + lbd0*deltaReg[i][j]\n",
    "        if val < 1: val = 1\n",
    "        if val > 5: val = 5\n",
    "        TopicIn[i][j] = val\n",
    "\n",
    "TopicInPd = []\n",
    "for r in xrange(num_test):\n",
    "    TopicInPd.append(TopicIn[test_user[r]][test_business[r]]) \n",
    "\n",
    "TopicInPd_rmse = mean_squared_error(test_rating, TopicInPd)  \n",
    "print TopicInPd_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Social Model</h1>\n",
    "We compare three models of Social model.\n",
    "\n",
    "cell-1: Based on the paper, we calculate the pagerank of each user from its relation impact, user similarity from real-ratings in train data and a relation matrix Tij = 1 if user i and j are friends, 0 other wise. \n",
    "\n",
    "relationship: $Tij = 1 if i and j are friends, 0 otherwise $\n",
    "\n",
    "relation impact: $Wi = \\frac{1.0}{1.0+ log(pagerank)}$\n",
    "\n",
    "user similarity: $Cos_{ij} = \\frac{rating\\_vector_i * rating\\_vector_j}{length(rating\\_vector_i) * length(rating\\_vector_j)}$\n",
    "\n",
    "cell-2\n",
    "$Ubb = mu - b_x - b_i$\n",
    "\n",
    "social-model1: $predicted(rating_i-Ubb) = \\frac{\\sum (rating_j-Ubb)}{Num(total friends)}$ if j is a friend of i\n",
    "\n",
    "social-model2: $predicted(rating_i-Ubb) = \\frac{\\sum Social\\_impact*(rating_j-Ubb)}{Num(total user)}$ for all user as j\n",
    "\n",
    "social-model3: $predicted(rating_i-Ubb) = \\frac{\\sum User\\_similarity*(rating_j-Ubb)}{Num(total user)}$ for all user as j\n",
    "\n",
    "cell-3: calculation of rmse. It turns out that using VIP users to calculate rating has least rmse, that is to say using social impact to do recommendation is most accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "\n",
    "Rij = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "Sij = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "Tij = csr_matrix((val,(row,col)), shape=(num_user,num_user)).toarray()\n",
    "Uij = csr_matrix((val,(row,col)), shape=(num_user,num_user)).toarray()\n",
    "Vij = csr_matrix((val,(row,col)), shape=(num_user,num_user)).toarray()\n",
    "\n",
    "# user-item rating matrix. If ui gives a rating to vj, Rij is the rating score, otherwise 0\n",
    "for r in xrange(num_train):\n",
    "    Rij[train_user[r]][train_business[r]] = train_rating[r] - Ubb[train_user[r]][train_business[r]]\n",
    "    Sij[train_user[r]][train_business[r]] = 1\n",
    "       \n",
    "# user-user social relations where Tij = 1 if ui,uj has a relation and zero otherwise\n",
    "for u in relation:\n",
    "    for f in u[1]:\n",
    "        Tij[u[0]][f] = 1\n",
    "\n",
    "# VIP matrix where Uij = 1 if uj is a VIP and zero otherwise        \n",
    "PR = {}\n",
    "for u in relation:\n",
    "    PR[u[0]] = len(u[1])    \n",
    "sorted_PR = sorted(PR.items(), key=itemgetter(1), reverse=True)\n",
    "rank = {}\n",
    "for u in xrange(num_user):\n",
    "    rank[sorted_PR[u][0]] = u+1\n",
    "Wi = []\n",
    "for u in xrange(num_user):\n",
    "    Wi.append(1.0/(1.0+ math.log(rank[u])))\n",
    "\n",
    "for u in xrange(num_user):\n",
    "    for v in xrange(num_user):\n",
    "        Uij[u][v] = Wi[v]\n",
    "        Vij[u][v] = 1\n",
    "\n",
    "# user-user similarity\n",
    "Cos_norm = []\n",
    "for u in xrange(num_user):\n",
    "    Cos_norm.append(math.sqrt(np.dot(Rij[u], Rij[u])))\n",
    "\n",
    "Cos = np.dot(Rij, Rij.T)\n",
    "for i in xrange(num_user):\n",
    "    for j in xrange(num_user):\n",
    "        if Cos_norm[i] and Cos_norm[j]:\n",
    "            Cos[i][j] = Cos[i][j]/Cos_norm[i]/Cos_norm[j]\n",
    "        else: \n",
    "            Cos[i][j] = 0\n",
    "\n",
    "TR = np.dot(Tij, Rij)\n",
    "TS = np.dot(Tij, Sij)\n",
    "UR = np.dot(Uij, Rij)\n",
    "VS = np.dot(Uij, Sij)\n",
    "CR = np.dot(Cos, Rij)\n",
    "\n",
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "Vij1 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "Vij2 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "Vij3 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "\n",
    "for i in xrange(num_user):\n",
    "    for j in xrange(num_business):\n",
    "        if TS[i][j]: Vij1[i][j] = TR[i][j]/TS[i][j]\n",
    "        if VS[i][j]: Vij2[i][j] = UR[i][j]/VS[i][j]\n",
    "        if num_user: Vij3[i][j] = CR[i][j]/num_user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "SocialIn1 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "SocialIn2 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "SocialIn3 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "\n",
    "for i in xrange(num_user):\n",
    "    for j in xrange(num_business):\n",
    "        val1 = Ubb[i][j] + lbd1*Vij1[i][j]\n",
    "        val2 = Ubb[i][j] + lbd2*Vij2[i][j]\n",
    "        val3 = Ubb[i][j] + lbd3*Vij3[i][j]\n",
    "        \n",
    "        if val1 < 1: val1 = 1\n",
    "        if val1 > 5: val1 = 5\n",
    "        if val2 < 1: val2 = 1\n",
    "        if val2 > 5: val2 = 5\n",
    "        if val3 < 1: val3 = 1\n",
    "        if val3 > 5: val3 = 5\n",
    "            \n",
    "        SocialIn1[i][j] = val1\n",
    "        SocialIn2[i][j] = val2\n",
    "        SocialIn3[i][j] = val3\n",
    "\n",
    "SocialInPd1 = []\n",
    "SocialInPd2 = []\n",
    "SocialInPd3 = []\n",
    "for r in xrange(num_test):\n",
    "    SocialInPd1.append(SocialIn1[test_user[r]][test_business[r]]) \n",
    "    SocialInPd2.append(SocialIn2[test_user[r]][test_business[r]]) \n",
    "    SocialInPd3.append(SocialIn3[test_user[r]][test_business[r]]) \n",
    "\n",
    "SocialInPd_rmse1 = mean_squared_error(test_rating, SocialInPd1)  \n",
    "SocialInPd_rmse2 = mean_squared_error(test_rating, SocialInPd2)  \n",
    "SocialInPd_rmse3 = mean_squared_error(test_rating, SocialInPd3)  \n",
    "\n",
    "print \"relation rmse =\", SocialInPd_rmse1\n",
    "print \"VIP rmse =\", SocialInPd_rmse2\n",
    "print \"similarity rmse =\", SocialInPd_rmse3\n",
    "\n",
    "if SocialInPd_rmse1 < SocialInPd_rmse2 and SocialInPd_rmse1 < SocialInPd_rmse3:\n",
    "    print \"relation between users dominant rating\"\n",
    "    SocialInPd_rmse = SocialInPd_rmse1\n",
    "elif SocialInPd_rmse2 < SocialInPd_rmse3:\n",
    "    print \"VIP user dominant rating\"\n",
    "    SocialInPd_rmse = SocialInPd_rmse2\n",
    "else:\n",
    "    print \"user similarity dominant rating\"\n",
    "    SocialInPd_rmse = SocialInPd_rmse3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tri-Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tri-Model is a linear combination of Topic model and social model.\n",
    "\n",
    "$$predicted\\_rating = Ubb + lbd4 * Topic\\_predicted(ratingi−Ubb) +\n",
    "lbd5 * Social\\_predicted(ratingi−Ubb)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "Tri = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "\n",
    "for i in xrange(num_user):\n",
    "    for j in xrange(num_business):\n",
    "        val = Ubb[i][j] + lbd4*deltaReg[i][j] + lbd5*Vij2[i][j]\n",
    "        \n",
    "        if val1 < 1: val1 = 1\n",
    "        if val1 > 5: val1 = 5\n",
    "        if val2 < 1: val2 = 1\n",
    "        if val2 > 5: val2 = 5\n",
    "        if val3 < 1: val3 = 1\n",
    "        if val3 > 5: val3 = 5\n",
    "            \n",
    "        Tri[i][j] = val\n",
    "\n",
    "TriPd = []\n",
    "for r in xrange(num_test):\n",
    "    TriPd.append(Tri[test_user[r]][test_business[r]]) \n",
    "\n",
    "TriPd_rmse = mean_squared_error(test_rating, TriPd)  \n",
    "print TriPd_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Disscussion</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Ubb_rmse =\", Ubb_rmse\n",
    "print \"Basic-model_rmse =\", BasicPd_rmse\n",
    "print \"TopicIn-model_rmse =\", TopicInPd_rmse\n",
    "print \"Social-model_rmse =\", SocialInPd_rmse\n",
    "print \"Tri-model_rmse =\", TriPd_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>10 fold results and parameter optimazation</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./bin/fig-lbd0.png\">\n",
    "<img src=\"./bin/fig-lbd1.png\">\n",
    "<img src=\"./bin/fig-lbd2.png\">\n",
    "<img src=\"./bin/fig-lbd3.png\">\n",
    "<img src=\"./bin/fig-10fold.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
