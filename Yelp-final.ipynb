{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Collection </h1>\n",
    "\n",
    "Data Source: Yelp Dataset Challenge https://www.yelp.com/dataset_challenge, main referrence: 'A Synthetic Approach for Recommendation: Combining Ratings, Social Relations, and Reviews', Guang-Neng Hu1, Xin-Yu Dai1, Yunya Song2, Shu-Jian Huang1, Jia-Jun Chen1\n",
    "\n",
    "The original file is too large, we seperate the original json file into smaller file, each file only include one feature.\n",
    "\n",
    "Below is an example how we do data extraction. The json file is too large and it is NOT uploaded to GitHub. We run data collection on Brazos Cluster.\n",
    "\n",
    "See more in ./preprocess/data-extraction/DATA.README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PreProcess = False\n",
    "if PreProcess:\n",
    "    location = []\n",
    "    fInput = open(\"./yelp_academic_dataset_business.json\",'r')\n",
    "    for line in fInput:\n",
    "        txt = \"[\" + line.rstrip() + \"]\"\n",
    "        json_txt = json.loads(txt)\n",
    "        location.append([json_txt[0][\"business_id\"], json_txt[0][\"latitude\"], json_txt[0][\"longitude\"], json_txt[0][\"city\"], json_txt[0][\"state\"], json_txt[0][\"postal_code\"]])\n",
    "    fInput.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geo-clustering\n",
    "\n",
    "First we extracted location information from Yelp business data and clustered those business using DBSCAN library. We got 126 clusters. We choose a cluster Longtitude between -81.4 and -81.4, Latitude between 34.8 and 35.6.\n",
    "\n",
    "\n",
    "It takes some time to run clustering. \n",
    "See more details in ./preprocess/location-cluster/DBSCAN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RunCluster = False\n",
    "if RunCluster:\n",
    "    import pandas as pd, numpy as np, matplotlib.pyplot as plt, time\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    from sklearn import metrics\n",
    "    from geopy.distance import great_circle\n",
    "    from shapely.geometry import MultiPoint\n",
    "\n",
    "    kms_per_radian = 6371.0088\n",
    "    \n",
    "    df = pd.read_csv('./preprocess/location-cluster/local2.csv', encoding='utf-8')\n",
    "    df.head()\n",
    "\n",
    "    coords = df.as_matrix(columns=['lat', 'lon'])\n",
    "\n",
    "    # define epsilon as 1.5 kilometers to have a middle size cluster size\n",
    "    epsilon = 1.5 / kms_per_radian\n",
    "\n",
    "    db = DBSCAN(eps=epsilon, min_samples=1, algorithm='ball_tree', metric='haversine').fit(np.radians(coords))\n",
    "    cluster_labels = db.labels_\n",
    "    num_clusters = len(set(cluster_labels))\n",
    "    clusters = pd.Series([coords[cluster_labels==n] for n in range(num_clusters)])\n",
    "\n",
    "    def get_centermost_point(cluster):\n",
    "        centroid = (MultiPoint(cluster).centroid.x, MultiPoint(cluster).centroid.y)\n",
    "        centermost_point = min(cluster, key=lambda point: great_circle(point, centroid).m)\n",
    "        return tuple(centermost_point)\n",
    "    centermost_points = clusters.map(get_centermost_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./preprocess/fig/Distribution-All.png\">\n",
    "<img src=\"./preprocess/fig/Distribution-local2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review statistics\n",
    "\n",
    "Then we plot the user vs. number of review and business vs. number of review. Result showed below.\n",
    "\n",
    "Based on statistical results, user/business with review number less than 30 have a large proportion while these user/business review has little help with recommendation (only make the recomendation matrix sparse). So we decided to analysis user/business whose review number is more than 30.\n",
    "\n",
    "Since we will build a model based on ratings, reviews and relations. We also exclude user with less than 30 friends from our data to make the relation network more intensive. The final dataset used for our project is in ../new_5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_user = False\n",
    "plot_business = False\n",
    "\n",
    "import math\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "u = open('./preprocess/user+business_distribution/user-stat.out', 'r').read()\n",
    "user = eval(u)\n",
    "b = open('./preprocess/user+business_distribution/business-stat.out', 'r').read()\n",
    "business = eval(b)\n",
    "\n",
    "review_count  = user[0]\n",
    "average_stars = user[5]\n",
    "reviews = business[0]\n",
    "stars = business[1]\n",
    "\n",
    "if plot_user:\n",
    "    review_count_log = {}\n",
    "    for k in review_count: review_count_log[int(k/30)] = 0 \n",
    "    for k in review_count: review_count_log[int(k/30)] += review_count[k]\n",
    "    for k in review_count_log: review_count_log[k] = 1.0*math.log(review_count_log[k]+1.0)\n",
    "    xname = \"count(review)/30\"\n",
    "    yname = \"log( count(user) )\"\n",
    "    title_name = \"distribution of reviews per user\"\n",
    "    fig_name = \"user_review_count\"\n",
    "\n",
    "    lists = sorted(review_count_log.items()) \n",
    "    x, y = zip(*lists) \n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(xname)\n",
    "    plt.ylabel(yname)\n",
    "    plt.title(title_name)\n",
    "    plt.savefig(fig_name)\n",
    "    plt.close('all')\n",
    "\n",
    "if plot_business:\n",
    "    review_count_log = {}\n",
    "    for k in reviews: review_count_log[int(k/30)] = 0 \n",
    "    for k in reviews: review_count_log[int(k/30)] += reviews[k]\n",
    "    for k in review_count_log: review_count_log[k] = 1.0*math.log(review_count_log[k]+1.0)\n",
    "    xname = \"count(review)/30\"\n",
    "    yname = \"log( count(business) )\"\n",
    "    title_name = \"distribution of reviews per business\"\n",
    "    fig_name = \"business_review_count\"\n",
    "    lists = sorted(review_count_log.items()) \n",
    "    x, y = zip(*lists) \n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(xname)\n",
    "    plt.ylabel(yname)\n",
    "    plt.title(title_name)\n",
    "    plt.savefig(fig_name)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./preprocess/fig/user_review_count.png\">\n",
    "<img src=\"./preprocess/fig/business_review_count.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Recommendation System </h1>\n",
    "\n",
    "File 5k-data stores all business_ID, business_avg_rating, user_ID, and user_avg_rating.\n",
    "\n",
    "File 5k-relation stores all users and their friends.\n",
    "\n",
    "File 5k-review stores all business_ID(review_to), user_ID(review_by), and rating and review_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('./new_5k/5k-data', 'r')\n",
    "business_name = eval(f.readline())\n",
    "business_avg = eval(f.readline())\n",
    "user_name = eval(f.readline())\n",
    "user_avg = eval(f.readline())\n",
    "\n",
    "f = open('./new_5k/5k-relation', 'r').read()\n",
    "relation = eval(f)\n",
    "\n",
    "f = open('./new_5k/5k-review', 'r')\n",
    "review5k_business = eval(f.readline())\n",
    "review5k_user = eval(f.readline())\n",
    "review5k_rating = eval(f.readline())\n",
    "review5k_text = eval(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import math\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Parameters</h1>\n",
    "\n",
    "Below, we run the model once to show how our model works using the first 10% as test data and the rest 90% as train data. We alse use 10-fold method to select parameters. For more details about the 10-fold model, please refer to\n",
    "./bin/TopicMF-10fold-YY.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_user = []\n",
    "train_business = []\n",
    "train_rating = []\n",
    "train_text = []\n",
    "\n",
    "test_user = []\n",
    "test_business = []\n",
    "test_rating = []\n",
    "\n",
    "for r in xrange(len(review5k_rating)):\n",
    "    if r < len(review5k_rating)/10:\n",
    "        test_user.append(review5k_user[r])\n",
    "        test_business.append(review5k_business[r])\n",
    "        test_rating.append(review5k_rating[r])\n",
    "    else:\n",
    "        train_user.append(review5k_user[r])\n",
    "        train_business.append(review5k_business[r])\n",
    "        train_rating.append(review5k_rating[r])\n",
    "        train_text.append(review5k_text[r])\n",
    "        \n",
    "K_topic = 10\n",
    "Times = 50\n",
    "DocWord = 300\n",
    "DocTopic = 5\n",
    "\n",
    "lbd0 = 0.4 # Topic effect\n",
    "lbd1 = 0.5 # Similarity effect \n",
    "lbd2 = 0.5 # relation afftect\n",
    "lbd3 = 0.5 # VIP effect\n",
    "lbd4 = 0.5 # Topic effect in Tri-model\n",
    "lbd5 = 0.5 # VIP effect in Tri-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_user = len(user_avg)\n",
    "num_business = len(business_avg)\n",
    "num_train = len(train_rating)\n",
    "num_test = len(test_rating)\n",
    "\n",
    "mu = np.mean(train_rating)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = get_stop_words('en')\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "def prep(doc):\n",
    "    raw = doc.lower().replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    texts = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    return (\" \").join(texts)\n",
    "\n",
    "from sklearn import linear_model\n",
    "def MLR(X, Y):\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(X, Y)\n",
    "    return reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Basic Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting subset of data, which we can compute in limited time and large enough for prediction, the first model we use is the basic SVD model. Officially this model is stated as:\n",
    "\n",
    "$${R}_{i,x}=\\mu + b_i + b_x + U_i^T \\dot V_x$$\n",
    "\n",
    "here, we get *U* and *V* from:\n",
    "\n",
    "$$Min_{U,V} \\sum ((r_{i,x}-R_{i,x})^2 + \\lambda ||U||^2 +\\lambda ||V||^2)$$\n",
    "\n",
    "$r_{i,x}$ is the training data, $R_{i,x}$ is the prediction data, $\\lambda$ is the regularization parameter.\n",
    "\n",
    "The dimention of $U$ is (#users, $k$), $V$ is (#restaurants, $k$), here $k$ is number of laten factors. To determine the proper $k$ for basic model, please refer to *bin/XL_part.ipython*, the *'Basic Model'* Part. The best $k$ for basic model is 30, which is big enough to minimize the RMSE of the training data and minimizes the possible computation assumption; The regularization parameter $\\lambda$ is 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ubb_rmse = 0.804720807844\n"
     ]
    }
   ],
   "source": [
    "Ubb = []\n",
    "for i in xrange(num_user):\n",
    "    Ubb.append([])\n",
    "    for j in xrange(num_business):\n",
    "        val = user_avg[i] + business_avg[j] - mu\n",
    "        if val < 1: val = 1\n",
    "        if val > 5: val = 5\n",
    "        Ubb[i].append(val) \n",
    "        \n",
    "UbbPd = []\n",
    "for r in xrange(num_test):\n",
    "    UbbPd.append(Ubb[test_user[r]][test_business[r]]) \n",
    "\n",
    "Ubb_rmse = mean_squared_error(test_rating, UbbPd)  \n",
    "\n",
    "print \"Ubb_rmse =\", Ubb_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicPd_rmse = 0.800139376144\n"
     ]
    }
   ],
   "source": [
    "BasicIn = []\n",
    "for i in xrange(num_user):\n",
    "    BasicIn.append([])\n",
    "    for j in xrange(num_business):\n",
    "        val = user_avg[i] + business_avg[j] - mu\n",
    "        if val < 1: val = 1\n",
    "        if val > 5: val = 5\n",
    "        BasicIn[i].append(val) \n",
    "        \n",
    "for r in xrange(num_train):\n",
    "    BasicIn[train_user[r]][train_business[r]] = train_rating[r]\n",
    "model = NMF(n_components=K_topic, init='random', random_state=0)\n",
    "U = model.fit_transform(BasicIn);\n",
    "V = model.components_;\n",
    "BasicOut = np.dot(U,V)\n",
    "BasicPd = []\n",
    "for r in xrange(num_test):\n",
    "    BasicPd.append(BasicOut[test_user[r]][test_business[r]]) \n",
    "\n",
    "BasicPd_rmse = mean_squared_error(test_rating, BasicPd)  \n",
    "\n",
    "print \"BasicPd_rmse =\", BasicPd_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Topic Model</h1>\n",
    "\n",
    "Firstly, the topics we obtain can explain the variation present in ratings and reviews. Secondly, combining ratings with review text allows us to predict ratings more accurately than approaches that consider either of the two sources of data in isolation.\n",
    "\n",
    "### model in the paper\n",
    "To get the topic distribution of reviews and words, LDA (Latent Dirichlet Allocation) is used. After the review-topic and word-topic distribution matrix. For a particular review, given the word-topic distribution $\\phi$ and topic assignment for each word $z_{d,j}$, (topic assignment for word $j$ in review $d$) and the topic distribution $\\theta$, the probability of a paticualar review is:\n",
    "\n",
    "$$p(\\Gamma|\\theta,\\phi,z)=\\Pi_{d\\in \\Gamma} \\Pi_{j=1}^{N_d} \\theta _{z_{d,j}} \\phi_{z_{d,j},w_{d,j}}$$\n",
    "\n",
    "The Topic MF model is defined as:\n",
    "\n",
    "$$Min_{U,V,\\theta,z} \\sum(r_{i,x}-R_{i,x})^2+\\lambda p(\\Gamma|\\theta,\\phi,z)$$\n",
    "\n",
    "Here $\\lambda$ is the regularization parameter for topic model, to prevent downflow, we use the format of $log$ likeliyhood:\n",
    "\n",
    "$$Min_{U,V,\\theta,z} \\sum(r_{i,x}-R_{i,x})^2 - \\lambda log(p(\\Gamma|\\theta,\\phi,z))$$,\n",
    "\n",
    "That is:\n",
    "\n",
    "$$Min_{U,V,\\theta,z} \\sum(r_{i,x}-R_{i,x})^2 - \\lambda \\sum_{d\\in \\Gamma} \\sum_{j=1}^{N_d}log(\\theta _{z_{d,j}} \\phi_{z_{d,j},w_{d,j}})$$,\n",
    "\n",
    "For Topic model of the paper, a C++ program is used to evaluate, the results are shown in /bin/XL_Part, the *'Topic Model'* part, after parameter adjust, *the best RMSE is 0.907157138867*\n",
    "\n",
    "### model modified by us\n",
    "cell-1: For the topic model, we first preprocessed all reviews (including stemming, removing stop-words...). \n",
    "\n",
    "cell-2: For each business, we grouped all its reviews in training data as bag-of-words, and then use the LDA library to calculate the topic distribution for each business. To shoeter calculation time, we choose 5 topics.\n",
    "\n",
    "|      | topic 1| topic2 | topic3 | ... |\n",
    "|------|--------|--------|--------|-----|\n",
    "| doc1 | 0.0513 | 0.4686 | 0.0092 | ... |\n",
    "| doc2 | 0.0006 | 0.8601 | 0.0006 | ... |\n",
    "| doc3 | 0.9989 | 0.0003 | 0.0003 | ... |\n",
    "| ...  | ...    | ...    | ...    | ... |\n",
    "\n",
    "cell-3: For each user, we calculate his/her topic rating using linear regression based on his/her real-ratings in traing data substract Ubb. We mannually add six points to the linear regression in order to scale down the regression results. X1 = [0,0,0,0,0] Y1 = 0, X2 = [1,0,0,0,0] Y2 = 0, X3 = [0,1,0,0,0] Y3 = 0, X4 = [0,0,1,0,0] Y4 = 0, X5 = [0,0,0,1,0] Y5 = 0, X6 = [0,0,0,0,1] Y6 = 0\n",
    "\n",
    "\n",
    "$$delta = real\\_rating - mu - b_x - b_i$$\n",
    "$$predicted(rating_i−Ubb) = delta\\_reg = coef1*topic1 + coef2*topic2 + ...$$\n",
    "\n",
    "cell-4: calculation of rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Breview = [\"\"]*num_business\n",
    "for r in xrange(num_train):\n",
    "    Breview[train_business[r]] += prep(train_text[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic distribution for the first business is\n",
      "[0.051297192918605114, 0.46863790267242639, 0.0091755198199965308, 0.022794506341032496, 0.44809487824793953]\n"
     ]
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=DocWord, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(Breview)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=DocTopic, max_iter=5, learning_method='online',learning_offset=50.,random_state=0)\n",
    "DocTopDist = lda.fit_transform(tf)\n",
    "print \"topic distribution for the first business is\"\n",
    "print list(DocTopDist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "\n",
    "# delta -> difference between rating and ubb [u*b], delta2 -> rating boolean\n",
    "delta = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "delta2 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "for r in xrange(num_train):\n",
    "    delta[train_user[r]][train_business[r]] = train_rating[r] - Ubb[train_user[r]][train_business[r]]\n",
    "    delta2[train_user[r]][train_business[r]] = 1\n",
    "\n",
    "deltaReg = []\n",
    "for i in xrange(num_user):\n",
    "    X = [[0,0,0,0,0], [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1]]; Y = [0,0,0,0,0,0]\n",
    "    for j in xrange(num_business):\n",
    "        if delta2[i][j]:\n",
    "            X.append(list(DocTopDist[j]))\n",
    "            Y.append(delta[i][j])\n",
    "    coef = MLR(X, Y)\n",
    "    deltaReg.append(np.dot(DocTopDist, coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.793822235553\n"
     ]
    }
   ],
   "source": [
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "TopicIn = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "for i in xrange(num_user):\n",
    "    for j in xrange(num_business):\n",
    "        val = Ubb[i][j] + lbd0*deltaReg[i][j]\n",
    "        if val < 1: val = 1\n",
    "        if val > 5: val = 5\n",
    "        TopicIn[i][j] = val\n",
    "\n",
    "TopicInPd = []\n",
    "for r in xrange(num_test):\n",
    "    TopicInPd.append(TopicIn[test_user[r]][test_business[r]]) \n",
    "\n",
    "TopicInPd_rmse = mean_squared_error(test_rating, TopicInPd)  \n",
    "print TopicInPd_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Social Model</h1>\n",
    "\n",
    "### why social mf:\n",
    "The pervasive usage of social media allows users to participate in online activities which produce a large amount of social relations such as trust relations in Epinions. Users in the physical world are likely to ask for suggestions from their local friends while they also tend to seek suggestions from users with high global reputations, indicating both local and global views of social relations can be potentially exploited to improve the performance of online recommender systems.\n",
    "\n",
    "### Model in the paper\n",
    "\n",
    "*exploiting local social context:*\n",
    "Users with similar tastes are more likely to be socially connected, and social influence suggests that users that are socially connected are more likely to share similar tastes. However, the low cost of social relation formation can lead to social relations with heterogeneous strengths. Since users with strong ties are more likely to share similar tastes than those with weak ties, treating all social relations equally is likely to lead to degradation in recommendation performance. We should\n",
    "consider heterogeneous strengths when exploiting local social context for recommendation. We simply use the rating cosine similarity to measure the social relation strength. The local social model is below:\n",
    "\n",
    "$$Min_{U,H} \\sum_{i=1}^n \\sum_{U_k \\in N_i} (S_{i,k}-U_i^THU_k)^2$$\n",
    "\n",
    "Here $U$ is the similar one in basic model. $H$ is the importance matrx. $S_{i,k}$ is the cosine similarity of user $i$, $k$, $N_i$ is the list of friends of user $i$\n",
    "*exploiting global social context:*\n",
    "user reputation plays an important role in recommendation and many companies employ people with high reputations to enhance consumers’ awareness and understanding of their products. Suggestions from people with high reputations positively affect a consumer’s adoption of a brand. Hence, global model is below:\n",
    "\n",
    "$$Min_{U,V} \\sum_{i,x} w_i (R_{i,x}-U_i^T V_x)^2$$\n",
    "\n",
    "Here, $w_i$ indicates the importance of a user, Page rank is a brilliant way to calculate the importance of pages, similarly, we can apply it into the social graph and compute the importance of users. $w_i$ can be defined as:\n",
    "\n",
    "$$w_i=\\frac{1}{1+log(r_i)}$$\n",
    "\n",
    "Here, $r_i$ is the page rank order of user $i$.\n",
    "\n",
    "*combination of global and local context:*\n",
    "\n",
    "$$Min_{U,H,V} \\sum_{i,x} w_i (R_{i,x}-U_i^T V_x)^2 + \\sum_{i=1}^n \\sum_{U_k \\in N_i} (S_{i,k}-U_i^THU_k)^2$$.\n",
    "\n",
    "Refer to */bin/XL_Part.ipython*, the *'Social Model'* part. The RMSE of paper model is *1.04159139513*.\n",
    "\n",
    "### model modified by us\n",
    "\n",
    "We compare three models of Social model.\n",
    "\n",
    "cell-1: Based on the paper, we calculate the pagerank of each user from its relation impact, user similarity from real-ratings in train data and a relation matrix Tij = 1 if user i and j are friends, 0 other wise. \n",
    "\n",
    "relationship: $Tij = 1$ if i and j are friends, 0 otherwise \n",
    "\n",
    "relation impact: $Wi = \\frac{1.0}{1.0+ log(pagerank)}$\n",
    "\n",
    "user similarity: $Cos_{ij} = \\frac{rating\\_vector_i * rating\\_vector_j}{length(rating\\_vector_i) * length(rating\\_vector_j)}$\n",
    "\n",
    "cell-2\n",
    "$Ubb = mu - b_x - b_i$\n",
    "\n",
    "social-model1: $predicted(rating_i-Ubb) = \\frac{\\sum (rating_j-Ubb)}{Num(total friends)}$ if j is a friend of i\n",
    "\n",
    "social-model2: $predicted(rating_i-Ubb) = \\frac{\\sum Social\\_impact*(rating_j-Ubb)}{Num(total user)}$ for all user as j\n",
    "\n",
    "social-model3: $predicted(rating_i-Ubb) = \\frac{\\sum User\\_similarity*(rating_j-Ubb)}{Num(total user)}$ for all user as j\n",
    "\n",
    "cell-3: calculation of rmse. It turns out that using VIP users to calculate rating has least rmse, that is to say using social impact to do recommendation is most accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "\n",
    "Rij = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "Sij = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "Tij = csr_matrix((val,(row,col)), shape=(num_user,num_user)).toarray()\n",
    "Uij = csr_matrix((val,(row,col)), shape=(num_user,num_user)).toarray()\n",
    "Vij = csr_matrix((val,(row,col)), shape=(num_user,num_user)).toarray()\n",
    "\n",
    "# user-item rating matrix. If ui gives a rating to vj, Rij is the rating score, otherwise 0\n",
    "for r in xrange(num_train):\n",
    "    Rij[train_user[r]][train_business[r]] = train_rating[r] - Ubb[train_user[r]][train_business[r]]\n",
    "    Sij[train_user[r]][train_business[r]] = 1\n",
    "       \n",
    "# user-user social relations where Tij = 1 if ui,uj has a relation and zero otherwise\n",
    "for u in relation:\n",
    "    for f in u[1]:\n",
    "        Tij[u[0]][f] = 1\n",
    "\n",
    "# VIP matrix where Uij = 1 if uj is a VIP and zero otherwise        \n",
    "PR = {}\n",
    "for u in relation:\n",
    "    PR[u[0]] = len(u[1])    \n",
    "sorted_PR = sorted(PR.items(), key=itemgetter(1), reverse=True)\n",
    "rank = {}\n",
    "for u in xrange(num_user):\n",
    "    rank[sorted_PR[u][0]] = u+1\n",
    "Wi = []\n",
    "for u in xrange(num_user):\n",
    "    Wi.append(1.0/(1.0+ math.log(rank[u])))\n",
    "\n",
    "for u in xrange(num_user):\n",
    "    for v in xrange(num_user):\n",
    "        Uij[u][v] = Wi[v]\n",
    "        Vij[u][v] = 1\n",
    "\n",
    "# user-user similarity\n",
    "Cos_norm = []\n",
    "for u in xrange(num_user):\n",
    "    Cos_norm.append(math.sqrt(np.dot(Rij[u], Rij[u])))\n",
    "\n",
    "Cos = np.dot(Rij, Rij.T)\n",
    "for i in xrange(num_user):\n",
    "    for j in xrange(num_user):\n",
    "        if Cos_norm[i] and Cos_norm[j]:\n",
    "            Cos[i][j] = Cos[i][j]/Cos_norm[i]/Cos_norm[j]\n",
    "        else: \n",
    "            Cos[i][j] = 0\n",
    "\n",
    "TR = np.dot(Tij, Rij)\n",
    "TS = np.dot(Tij, Sij)\n",
    "UR = np.dot(Uij, Rij)\n",
    "VS = np.dot(Uij, Sij)\n",
    "CR = np.dot(Cos, Rij)\n",
    "\n",
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "Vij1 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "Vij2 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "Vij3 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "\n",
    "for i in xrange(num_user):\n",
    "    for j in xrange(num_business):\n",
    "        if TS[i][j]: Vij1[i][j] = TR[i][j]/TS[i][j]\n",
    "        if VS[i][j]: Vij2[i][j] = UR[i][j]/VS[i][j]\n",
    "        if num_user: Vij3[i][j] = CR[i][j]/num_user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relation rmse = 0.796753384642\n",
      "VIP rmse = 0.773081349841\n",
      "similarity rmse = 0.804714677484\n",
      "VIP user dominant rating\n"
     ]
    }
   ],
   "source": [
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "SocialIn1 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "SocialIn2 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "SocialIn3 = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "\n",
    "for i in xrange(num_user):\n",
    "    for j in xrange(num_business):\n",
    "        val1 = Ubb[i][j] + lbd1*Vij1[i][j]\n",
    "        val2 = Ubb[i][j] + lbd2*Vij2[i][j]\n",
    "        val3 = Ubb[i][j] + lbd3*Vij3[i][j]\n",
    "        \n",
    "        if val1 < 1: val1 = 1\n",
    "        if val1 > 5: val1 = 5\n",
    "        if val2 < 1: val2 = 1\n",
    "        if val2 > 5: val2 = 5\n",
    "        if val3 < 1: val3 = 1\n",
    "        if val3 > 5: val3 = 5\n",
    "            \n",
    "        SocialIn1[i][j] = val1\n",
    "        SocialIn2[i][j] = val2\n",
    "        SocialIn3[i][j] = val3\n",
    "\n",
    "SocialInPd1 = []\n",
    "SocialInPd2 = []\n",
    "SocialInPd3 = []\n",
    "for r in xrange(num_test):\n",
    "    SocialInPd1.append(SocialIn1[test_user[r]][test_business[r]]) \n",
    "    SocialInPd2.append(SocialIn2[test_user[r]][test_business[r]]) \n",
    "    SocialInPd3.append(SocialIn3[test_user[r]][test_business[r]]) \n",
    "\n",
    "SocialInPd_rmse1 = mean_squared_error(test_rating, SocialInPd1)  \n",
    "SocialInPd_rmse2 = mean_squared_error(test_rating, SocialInPd2)  \n",
    "SocialInPd_rmse3 = mean_squared_error(test_rating, SocialInPd3)  \n",
    "\n",
    "print \"relation rmse =\", SocialInPd_rmse1\n",
    "print \"VIP rmse =\", SocialInPd_rmse2\n",
    "print \"similarity rmse =\", SocialInPd_rmse3\n",
    "\n",
    "if SocialInPd_rmse1 < SocialInPd_rmse2 and SocialInPd_rmse1 < SocialInPd_rmse3:\n",
    "    print \"relation between users dominant rating\"\n",
    "    SocialInPd_rmse = SocialInPd_rmse1\n",
    "elif SocialInPd_rmse2 < SocialInPd_rmse3:\n",
    "    print \"VIP user dominant rating\"\n",
    "    SocialInPd_rmse = SocialInPd_rmse2\n",
    "else:\n",
    "    print \"user similarity dominant rating\"\n",
    "    SocialInPd_rmse = SocialInPd_rmse3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tri-Model</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tri-Model is a linear combination of Topic model and social model.\n",
    "\n",
    "$$predicted\\_rating = Ubb + lbd4 * Topic\\_predicted(ratingi−Ubb) +\n",
    "lbd5 * Social\\_predicted(ratingi−Ubb)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765334713651\n"
     ]
    }
   ],
   "source": [
    "row = np.array([])\n",
    "col = np.array([])\n",
    "val = np.array([])\n",
    "Tri = csr_matrix((val,(row,col)), shape=(num_user,num_business)).toarray()\n",
    "\n",
    "for i in xrange(num_user):\n",
    "    for j in xrange(num_business):\n",
    "        val = Ubb[i][j] + lbd4*deltaReg[i][j] + lbd5*Vij2[i][j]\n",
    "        \n",
    "        if val1 < 1: val1 = 1\n",
    "        if val1 > 5: val1 = 5\n",
    "        if val2 < 1: val2 = 1\n",
    "        if val2 > 5: val2 = 5\n",
    "        if val3 < 1: val3 = 1\n",
    "        if val3 > 5: val3 = 5\n",
    "            \n",
    "        Tri[i][j] = val\n",
    "\n",
    "TriPd = []\n",
    "for r in xrange(num_test):\n",
    "    TriPd.append(Tri[test_user[r]][test_business[r]]) \n",
    "\n",
    "TriPd_rmse = mean_squared_error(test_rating, TriPd)  \n",
    "print TriPd_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Disscussion</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ubb_rmse = 0.804720807844\n",
      "Basic-model_rmse = 0.800139376144\n",
      "TopicIn-model_rmse = 0.793822235553\n",
      "Social-model_rmse = 0.773081349841\n",
      "Tri-model_rmse = 0.765334713651\n"
     ]
    }
   ],
   "source": [
    "print \"Ubb_rmse =\", Ubb_rmse\n",
    "print \"Basic-model_rmse =\", BasicPd_rmse\n",
    "print \"TopicIn-model_rmse =\", TopicInPd_rmse\n",
    "print \"Social-model_rmse =\", SocialInPd_rmse\n",
    "print \"Tri-model_rmse =\", TriPd_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>10 fold results and parameter optimazation</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./bin/fig-lbd0.png\">\n",
    "<img src=\"./bin/fig-lbd1.png\">\n",
    "<img src=\"./bin/fig-lbd2.png\">\n",
    "<img src=\"./bin/fig-lbd3.png\">\n",
    "<img src=\"./bin/fig-10fold.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
