{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "\n",
    "# Homework 1:  Retrieval Models: Boolean + Vector Space\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Thursday, February 2 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine. In particular, there are three main learning objectives: (i) the basics of tokenization (e.g. stemming, case-folding, etc.) and its effect on information retrieval; (ii) basics of index building and Boolean retrieval; and (iii) basics of the Vector Space model and ranked retrieval.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as lastname_firstinitial_hw#.ipynb. For example, my homework submission would be: caverlee_j_hw1.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Friday, February 5 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We provide the dataset, [southpark_scripts.zip](https://www.dropbox.com/s/6rzfsbn97s8vwof/southpark_scripts.zip), which includes scripts for episodes of the first twenty seasons of the TV show South Park. You will build an inverted index over these scripts where each episode should be treated as a single document. There are 277 episodes (documents) to index and search on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (20 points) Part 1: Parsing\n",
    "\n",
    "First, you should tokenize documents using **whitespaces and punctuations as delimiters** but do not remove stop words. Your parser needs to also provide the following two confgiuration options:\n",
    "* Case-folding\n",
    "* Stemming: use [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)\n",
    "\n",
    "Please note that you should stick to the stemming package listed above. Otherwise, given the same query, the results generated by your code can be different from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration options\n",
    "use_stemming = True # or false\n",
    "use_casefolding = True # or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your parser function here. It will take the two option variables above as the parameters\n",
    "# add cells as needed to organize your code\n",
    "\n",
    "def parser(use_stemming, use_casefolding):\n",
    "    import glob\n",
    "    import re\n",
    "    from nltk import stem\n",
    "    \n",
    "    documents=[]\n",
    "    names=[]\n",
    "    stemmer = stem.PorterStemmer()\n",
    "    rs=[];\n",
    "    for filename in glob.glob('*.txt'):\n",
    "        names.append(filename)\n",
    "        fl=open(filename)\n",
    "        pre_words=fl.read();\n",
    "        words=pre_words.decode(\"utf8\",'ignore')   #maybe something wrong here to ignore.\n",
    "        #word_list=re.split(\";\\W*|,\\W*|\\*\\W*|\\n\\W*|\\s+|-\\W*|\\.\\W*|\\:\\W*|\\?\\W*|!\\W*|_\\W*|\\'\\W*\",words)\n",
    "        word_list=re.split(\"\\W+\",words);\n",
    "        stemmed=[]\n",
    "        case_fold=[]\n",
    "        origin=[]\n",
    "        for word in word_list:\n",
    "            if not word:\n",
    "                continue\n",
    "            origin.append(word)\n",
    "            case_fold.append(word.lower())\n",
    "            if use_casefolding:\n",
    "                stemmed.append(stemmer.stem(word.lower()))\n",
    "            else:\n",
    "                stemmed.append(stemmer.stem(word))\n",
    "        if use_stemming:\n",
    "            documents.append(stemmed)\n",
    "        elif use_casefolding:\n",
    "            documents.append(case_fold)\n",
    "        else:\n",
    "            documents.append(origin)\n",
    "    return [documents, names];\n",
    "\n",
    "res=parser(use_stemming, use_casefolding)\n",
    "documents=res[0];\n",
    "names=res[1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Once you have your parser working, you should report here the size of your dictionary under the four cases. That is, how many unique tokens do you have with stemming on and casefolding on? And so on. You should fill in the following\n",
    "\n",
    "* Stemming + Casefolding       = 17143\n",
    "* Stemming + No Casefolding    = 22221\n",
    "* No Stemming + Casefolding    = 23806\n",
    "* No Stemming + No Casefolding = 29550\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 2: Boolean Retrieval\n",
    "\n",
    "In this part you build an inverted index to support Boolean retrieval. We only require your index to  support AND queries. In other words, your index does not have to support OR, NOT, or parentheses. Also, we do not explicitly expect to see AND in queries, e.g., when we query **great again**, your search engine should treat it as **great** AND **again**.\n",
    "\n",
    "Example queries:\n",
    "* Rednecks\n",
    "* Troll Trace\n",
    "* Respect my authority\n",
    "* Respect my authoritah\n",
    "* Respected my authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the index here\n",
    "# add cells as needed to organize your code\n",
    "mat={}\n",
    "num=len(documents)\n",
    "for i in xrange(num):\n",
    "    doc=documents[i];\n",
    "    for word in doc:\n",
    "        if word in mat:\n",
    "            mat[word][i]=True;\n",
    "        else:\n",
    "            mat[word]=[False]*num;\n",
    "            mat[word][i]=True;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk import stem\n",
    "stemmer = stem.PorterStemmer()\n",
    "\n",
    "search_text = raw_input('Boolean Search:')\n",
    "# search for the input using your index and print out ids of matching documents\n",
    "#search=re.split(\";\\W*|,\\W*|\\*\\W*|\\n\\W*|\\s+|-\\W*|\\.\\W*|\\:\\W*|\\?\\W*|!\\W*|_\\W*|\\'\\W*\",search_text)\n",
    "if search_text:\n",
    "    search=re.split(\"\\W+\",search_text);\n",
    "    size=len(search)\n",
    "    for i in xrange(size):\n",
    "        word=search[i]\n",
    "        if use_stemming and use_casefolding:\n",
    "            search[i]=stemmer.stem(word.lower());\n",
    "        if not use_stemming and use_casefolding:\n",
    "            search[i]=word.lower();\n",
    "        if use_stemming and not use_casefolding:\n",
    "            search[i]=stemmer.stem(word);\n",
    "    word=search[0];\n",
    "    rs=set();\n",
    "    if word in mat:\n",
    "        bool_array=np.array(mat[word])\n",
    "        indexes=np.where(bool_array==True)[0];\n",
    "        for idx in indexes:\n",
    "            rs.add(idx);\n",
    "        for idx in xrange(size):\n",
    "            word=search[idx];\n",
    "            if not word in mat:\n",
    "                rs.clear();\n",
    "                break;\n",
    "            bool_list=mat[word];\n",
    "            set_list=list(rs);\n",
    "            for ids in set_list:\n",
    "                if not bool_list[ids]:\n",
    "                    rs.remove(ids);\n",
    "            if len(rs)==0:\n",
    "                break;\n",
    "    for idx in rs:\n",
    "        print names[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "When we use stemming and casefolding, is the result different from the result when we do not use them? Do you find cases where you prefer stemming? Or not? Or cases where you prefer casefolding? Or Not? Write down your observations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are often different. When I want to look up for words with no concern of the grammer and just for the meanning itself, it's better to use stemming, but when we want to be more specific, we don't need then, such as, when we want to find 'bikes', use stemming, we will find 'bike' and 'bikes', which is a kind of interuption. As for casefolding, if we care about the big letter and small letter, it will make trouble, for example, when we want 'china', we will find 'China' as well, however, if we do not use it, for cases that words are at the beginning of the sentence, we will lose some results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Phrase Queries\n",
    "\n",
    "Your search engine needs to also (optionally) support phrase queries of arbitrary length. Use quotes in a query to tell your search engine this is a phrase query. Again, we don't explicitly type AND in queries and never use OR, NOT, or parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_text = raw_input('Boolean Search (Phrase Query:')\n",
    "# search for the input using your index and print out ids of matching documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 3: Ranked Retrieval\n",
    "\n",
    "In this part, your job is to support queries over an index that you build. This time you will use the vector space model plus cosine similarity to rank documents.\n",
    "\n",
    "**TFIDF:** For the document vectors, use the standard TFIDF scores. That is, use the log-weighted term frequency $1+log(tf)$; and the log-weighted inverse document frequency $log(\\frac{N}{df})$. For the query vector, use simple weights (the raw term frequency). For example:\n",
    "* query: troll $\\rightarrow$ (1)\n",
    "* query: troll trace $\\rightarrow$ (1, 1)\n",
    "\n",
    "**Output:**\n",
    "For a given query, you should rank all the 277 documents but you only need to output the top-5 documents (i.e. document ids) plus the cosine score of each of these documents. For example:\n",
    "\n",
    "* result1 - score1\n",
    "* result2 - score2\n",
    "* result3 - score3\n",
    "* result4 - score4\n",
    "* result5 - score5\n",
    "\n",
    "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the vector space index here\n",
    "# add cells as needed to organize your code\n",
    "import numpy as np\n",
    "import math\n",
    "mat={}\n",
    "num=len(documents)\n",
    "for i in xrange(num):\n",
    "    doc=documents[i];\n",
    "    for word in doc:\n",
    "        if word in mat:\n",
    "            mat[word][i]+=1.;\n",
    "        else:\n",
    "            mat[word]=[0.]*num;\n",
    "            mat[word][i]=1.;\n",
    "\n",
    "docf={};\n",
    "for word in mat:\n",
    "    count=0.;\n",
    "    for k in mat[word]:\n",
    "        if k!=0.:\n",
    "             count+=1.;\n",
    "    if count!=0.:\n",
    "        docf[word]=math.log10(num*1.0/count);\n",
    "    else:\n",
    "        docf[word]=0.;\n",
    "        \n",
    "for word in mat:\n",
    "    for i in xrange(num):\n",
    "        if mat[word][i]!=0.:\n",
    "            mat[word][i]=docf[word]*(1.+math.log10(mat[word][i]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import heapq as hp\n",
    "from nltk import stem\n",
    "stemmer = stem.PorterStemmer()\n",
    "import re\n",
    "import math\n",
    "\n",
    "now_size=0;\n",
    "search_text = raw_input('Ranked Search:')\n",
    "# search for the input and print the top 5 document ids along with their associated cosine scores.\n",
    "if search_text:\n",
    "    search=re.split(\"\\W+\",search_text);\n",
    "    h=[];\n",
    "    size=len(search)\n",
    "    filtered=[];\n",
    "    for i in xrange(size):\n",
    "        word=search[i]\n",
    "        if use_stemming and use_casefolding and stemmer.stem(word.lower()) in mat:\n",
    "            filtered.append(stemmer.stem(word.lower()));\n",
    "        if not use_stemming and use_casefolding and word.lower() in mat:\n",
    "            filtered.append(word.lower());\n",
    "        if use_stemming and not use_casefolding and stemmer.stem(word) in mat:\n",
    "            filtered.append(stemmer.stem(word));\n",
    "    now_size=len(filtered)\n",
    "    \n",
    "if now_size>0:\n",
    "    num=len(documents)\n",
    "    num_w=len(mat.keys())\n",
    "    v={};\n",
    "    for word in mat:\n",
    "        v[word]=0.;\n",
    "    for word in filtered:\n",
    "        v[word]+=1.;\n",
    "    v1=np.array([0.]*num_w);\n",
    "    idx=0;\n",
    "    for word in mat:\n",
    "        v1[idx]=v[word];\n",
    "        idx+=1;\n",
    "    x=math.sqrt(np.sum(v1*v1));\n",
    "    v1/=x;\n",
    "                \n",
    "    for i in xrange(num):\n",
    "        score=np.array([0.]*num_w);\n",
    "        idx=0;\n",
    "        for word in mat:\n",
    "            score[idx]=mat[word][i];\n",
    "            idx+=1;\n",
    "        le=math.sqrt(np.sum(score*score));\n",
    "        if le==0.:\n",
    "            hp.heappush(h,(0.,names[i]));\n",
    "        else:\n",
    "            fina=np.sum(v1*score)/le;\n",
    "            #print fina\n",
    "            hp.heappush(h,[fina,names[i]]);\n",
    "        if len(h)>5:\n",
    "            hp.heappop(h);\n",
    "\n",
    "    while len(h)>0:\n",
    "        entry=hp.heappop(h);\n",
    "        print entry[1],'-',entry[0] ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grader will randomly pick 5-10 queries to test your program. You are welcome to discuss the results returned by your search engine with others on Piazza."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
